{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from unityagents import UnityEnvironment\n",
    "from collections import deque\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from all_code import *\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name='../Reacher_Windows_x86_64/Reacher.exe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 20\n",
      "Size of each action: 4\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseTaskUnity:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def reset(self):\n",
    "        env_info = self.env.reset(train_mode=True)[brain_name]\n",
    "        return np.array(env_info.vector_observations)\n",
    "\n",
    "    def step(self, action):\n",
    "        env_info = self.env.step(action)[brain_name]\n",
    "        next_states = env_info.vector_observations\n",
    "        rewards = env_info.rewards\n",
    "        dones = env_info.local_done\n",
    "        if np.any(dones):\n",
    "            next_states = self.reset()\n",
    "        return np.array(next_states), np.array(rewards), np.array(dones), None\n",
    "\n",
    "    def seed(self, random_seed):\n",
    "        pass\n",
    "\n",
    "    \n",
    "class ReacherV1(BaseTaskUnity):\n",
    "    def __init__(self, name, log_dir=None):\n",
    "        BaseTaskUnity.__init__(self)\n",
    "        self.name = name\n",
    "        self.env = env\n",
    "        self.action_dim = brain.vector_action_space_size\n",
    "        self.state_dim = brain.vector_observation_space_size\n",
    "\n",
    "    def step(self, action):\n",
    "        return BaseTaskUnity.step(self, np.clip(action, -1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_steps_unity(agent):\n",
    "    np.random.seed()\n",
    "    torch.manual_seed(np.random.randint(int(1e6)))\n",
    "    config = agent.config\n",
    "    while True:\n",
    "        scores = agent.scores_deque\n",
    "        if len(agent.scores_list) > 0:\n",
    "            print('Episode {}\\tAverage Score Last {} Episodes: {:.3f}\\tAvg. Score (All Agents) Last Episode: {:.3f}'.format(len(agent.scores_list), len(scores), np.mean(scores), agent.scores_list[-1]))\n",
    "        if len(agent.scores_list) > 0 and len(agent.scores_list) % 100 == 0:\n",
    "            save_path = 'PPO-ReacherV2-checkpoint.bin'\n",
    "            agent.save(save_path)\n",
    "            print('Episode {}\\tAverage Score Last {} Episodes: {:.3f}'.format(len(agent.scores_list), len(scores),\n",
    "                np.mean(scores)))\n",
    "        if len(scores) and (np.mean(scores) >= 30.0):\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.3f}'.format(len(agent.scores_list)-100, np.mean(scores)))\n",
    "            save_path = 'PPO-ReacherV2-solved.bin'\n",
    "            agent.save(save_path)\n",
    "            res = True, agent.scores_deque, agent.scores_list, save_path\n",
    "            agent.close()\n",
    "            return res\n",
    "        if config.max_steps and agent.total_steps >= config.max_steps:\n",
    "            print('\\nMax episodes reached!\\tFinal Average Score: {:.3f}'.format(np.mean(scores)))\n",
    "            save_path = 'PPO-ReacherV2-max-steps.bin'\n",
    "            agent.close()\n",
    "            return False, None, None, None\n",
    "        agent.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppo_continuous_unity():\n",
    "    config = Config()\n",
    "    config.num_workers = num_agents\n",
    "    # task_fn = lambda log_dir: Pendulum(log_dir=log_dir)\n",
    "    # task_fn = lambda log_dir: Bullet('AntBulletEnv-v0', log_dir=log_dir)\n",
    "    task_fn = lambda: ReacherV1('ReacherV1')\n",
    "    config.task_fn = task_fn\n",
    "    config.state_dim = 33\n",
    "    config.action_dim = 4\n",
    "\n",
    "    config.network_fn = lambda: GaussianActorCriticNet(\n",
    "        config.state_dim, config.action_dim, actor_body=FCBody(config.state_dim),\n",
    "        critic_body=FCBody(config.state_dim))\n",
    "    config.optimizer_fn = lambda params: torch.optim.Adam(params, 3e-4, eps=1e-5)\n",
    "    config.discount = 0.99\n",
    "    config.use_gae = True\n",
    "    config.gae_tau = 0.95\n",
    "    config.gradient_clip = 5\n",
    "    config.rollout_length = 1024\n",
    "    config.optimization_epochs = 10\n",
    "    config.num_mini_batches = 256\n",
    "    config.ppo_ratio_clip = 0.2\n",
    "    config.log_interval = 2048\n",
    "    config.max_steps = 2e7\n",
    "    # config.logger = get_logger()\n",
    "    agent = PPOAgent(config)\n",
    "    return run_steps_unity(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1\tAverage Score Last 1 Episodes: 0.056\tAvg. Score (All Agents) Last Episode: 0.056\n",
      "Episode 2\tAverage Score Last 2 Episodes: 0.097\tAvg. Score (All Agents) Last Episode: 0.138\n",
      "Episode 3\tAverage Score Last 3 Episodes: 0.140\tAvg. Score (All Agents) Last Episode: 0.225\n",
      "Episode 4\tAverage Score Last 4 Episodes: 0.209\tAvg. Score (All Agents) Last Episode: 0.417\n",
      "Episode 5\tAverage Score Last 5 Episodes: 0.311\tAvg. Score (All Agents) Last Episode: 0.717\n",
      "Episode 6\tAverage Score Last 6 Episodes: 0.459\tAvg. Score (All Agents) Last Episode: 1.202\n",
      "Episode 7\tAverage Score Last 7 Episodes: 0.550\tAvg. Score (All Agents) Last Episode: 1.095\n",
      "Episode 8\tAverage Score Last 8 Episodes: 0.673\tAvg. Score (All Agents) Last Episode: 1.535\n",
      "Episode 9\tAverage Score Last 9 Episodes: 0.795\tAvg. Score (All Agents) Last Episode: 1.763\n",
      "Episode 10\tAverage Score Last 10 Episodes: 0.894\tAvg. Score (All Agents) Last Episode: 1.790\n",
      "Episode 11\tAverage Score Last 11 Episodes: 0.998\tAvg. Score (All Agents) Last Episode: 2.040\n",
      "Episode 12\tAverage Score Last 12 Episodes: 1.104\tAvg. Score (All Agents) Last Episode: 2.272\n",
      "Episode 13\tAverage Score Last 13 Episodes: 1.190\tAvg. Score (All Agents) Last Episode: 2.210\n",
      "Episode 14\tAverage Score Last 14 Episodes: 1.278\tAvg. Score (All Agents) Last Episode: 2.429\n",
      "Episode 15\tAverage Score Last 15 Episodes: 1.383\tAvg. Score (All Agents) Last Episode: 2.854\n",
      "Episode 16\tAverage Score Last 16 Episodes: 1.479\tAvg. Score (All Agents) Last Episode: 2.920\n",
      "Episode 17\tAverage Score Last 17 Episodes: 1.590\tAvg. Score (All Agents) Last Episode: 3.358\n",
      "Episode 18\tAverage Score Last 18 Episodes: 1.676\tAvg. Score (All Agents) Last Episode: 3.142\n",
      "Episode 19\tAverage Score Last 19 Episodes: 1.775\tAvg. Score (All Agents) Last Episode: 3.552\n",
      "Episode 20\tAverage Score Last 20 Episodes: 1.874\tAvg. Score (All Agents) Last Episode: 3.752\n",
      "Episode 21\tAverage Score Last 21 Episodes: 1.967\tAvg. Score (All Agents) Last Episode: 3.837\n",
      "Episode 22\tAverage Score Last 22 Episodes: 2.078\tAvg. Score (All Agents) Last Episode: 4.406\n",
      "Episode 23\tAverage Score Last 23 Episodes: 2.194\tAvg. Score (All Agents) Last Episode: 4.751\n",
      "Episode 24\tAverage Score Last 24 Episodes: 2.308\tAvg. Score (All Agents) Last Episode: 4.936\n",
      "Episode 25\tAverage Score Last 25 Episodes: 2.423\tAvg. Score (All Agents) Last Episode: 5.181\n",
      "Episode 26\tAverage Score Last 26 Episodes: 2.531\tAvg. Score (All Agents) Last Episode: 5.210\n",
      "Episode 27\tAverage Score Last 27 Episodes: 2.633\tAvg. Score (All Agents) Last Episode: 5.290\n",
      "Episode 28\tAverage Score Last 28 Episodes: 2.726\tAvg. Score (All Agents) Last Episode: 5.253\n",
      "Episode 29\tAverage Score Last 29 Episodes: 2.829\tAvg. Score (All Agents) Last Episode: 5.709\n",
      "Episode 30\tAverage Score Last 30 Episodes: 2.937\tAvg. Score (All Agents) Last Episode: 6.060\n",
      "Episode 31\tAverage Score Last 31 Episodes: 3.068\tAvg. Score (All Agents) Last Episode: 7.005\n",
      "Episode 32\tAverage Score Last 32 Episodes: 3.180\tAvg. Score (All Agents) Last Episode: 6.631\n",
      "Episode 33\tAverage Score Last 33 Episodes: 3.297\tAvg. Score (All Agents) Last Episode: 7.048\n",
      "Episode 34\tAverage Score Last 34 Episodes: 3.411\tAvg. Score (All Agents) Last Episode: 7.174\n",
      "Episode 35\tAverage Score Last 35 Episodes: 3.525\tAvg. Score (All Agents) Last Episode: 7.405\n",
      "Episode 36\tAverage Score Last 36 Episodes: 3.641\tAvg. Score (All Agents) Last Episode: 7.703\n",
      "Episode 37\tAverage Score Last 37 Episodes: 3.784\tAvg. Score (All Agents) Last Episode: 8.931\n",
      "Episode 38\tAverage Score Last 38 Episodes: 3.911\tAvg. Score (All Agents) Last Episode: 8.610\n",
      "Episode 39\tAverage Score Last 39 Episodes: 4.039\tAvg. Score (All Agents) Last Episode: 8.913\n",
      "Episode 40\tAverage Score Last 40 Episodes: 4.172\tAvg. Score (All Agents) Last Episode: 9.350\n",
      "Episode 41\tAverage Score Last 41 Episodes: 4.297\tAvg. Score (All Agents) Last Episode: 9.303\n",
      "Episode 42\tAverage Score Last 42 Episodes: 4.416\tAvg. Score (All Agents) Last Episode: 9.281\n",
      "Episode 43\tAverage Score Last 43 Episodes: 4.533\tAvg. Score (All Agents) Last Episode: 9.467\n",
      "Episode 45\tAverage Score Last 45 Episodes: 4.771\tAvg. Score (All Agents) Last Episode: 9.830\n",
      "Episode 46\tAverage Score Last 46 Episodes: 4.887\tAvg. Score (All Agents) Last Episode: 10.075\n",
      "Episode 47\tAverage Score Last 47 Episodes: 5.018\tAvg. Score (All Agents) Last Episode: 11.061\n",
      "Episode 48\tAverage Score Last 48 Episodes: 5.146\tAvg. Score (All Agents) Last Episode: 11.154\n",
      "Episode 49\tAverage Score Last 49 Episodes: 5.270\tAvg. Score (All Agents) Last Episode: 11.223\n",
      "Episode 50\tAverage Score Last 50 Episodes: 5.386\tAvg. Score (All Agents) Last Episode: 11.047\n",
      "Episode 51\tAverage Score Last 51 Episodes: 5.509\tAvg. Score (All Agents) Last Episode: 11.666\n",
      "Episode 52\tAverage Score Last 52 Episodes: 5.625\tAvg. Score (All Agents) Last Episode: 11.581\n",
      "Episode 53\tAverage Score Last 53 Episodes: 5.763\tAvg. Score (All Agents) Last Episode: 12.939\n",
      "Episode 54\tAverage Score Last 54 Episodes: 5.901\tAvg. Score (All Agents) Last Episode: 13.207\n",
      "Episode 55\tAverage Score Last 55 Episodes: 6.046\tAvg. Score (All Agents) Last Episode: 13.858\n",
      "Episode 56\tAverage Score Last 56 Episodes: 6.192\tAvg. Score (All Agents) Last Episode: 14.220\n",
      "Episode 57\tAverage Score Last 57 Episodes: 6.335\tAvg. Score (All Agents) Last Episode: 14.323\n",
      "Episode 58\tAverage Score Last 58 Episodes: 6.467\tAvg. Score (All Agents) Last Episode: 14.026\n",
      "Episode 59\tAverage Score Last 59 Episodes: 6.621\tAvg. Score (All Agents) Last Episode: 15.513\n",
      "Episode 60\tAverage Score Last 60 Episodes: 6.771\tAvg. Score (All Agents) Last Episode: 15.623\n",
      "Episode 61\tAverage Score Last 61 Episodes: 6.932\tAvg. Score (All Agents) Last Episode: 16.594\n",
      "Episode 62\tAverage Score Last 62 Episodes: 7.071\tAvg. Score (All Agents) Last Episode: 15.583\n",
      "Episode 63\tAverage Score Last 63 Episodes: 7.227\tAvg. Score (All Agents) Last Episode: 16.914\n",
      "Episode 64\tAverage Score Last 64 Episodes: 7.374\tAvg. Score (All Agents) Last Episode: 16.636\n",
      "Episode 65\tAverage Score Last 65 Episodes: 7.546\tAvg. Score (All Agents) Last Episode: 18.493\n",
      "Episode 66\tAverage Score Last 66 Episodes: 7.705\tAvg. Score (All Agents) Last Episode: 18.047\n",
      "Episode 67\tAverage Score Last 67 Episodes: 7.881\tAvg. Score (All Agents) Last Episode: 19.512\n",
      "Episode 68\tAverage Score Last 68 Episodes: 8.039\tAvg. Score (All Agents) Last Episode: 18.604\n",
      "Episode 69\tAverage Score Last 69 Episodes: 8.193\tAvg. Score (All Agents) Last Episode: 18.676\n",
      "Episode 70\tAverage Score Last 70 Episodes: 8.362\tAvg. Score (All Agents) Last Episode: 20.055\n",
      "Episode 71\tAverage Score Last 71 Episodes: 8.567\tAvg. Score (All Agents) Last Episode: 22.921\n",
      "Episode 72\tAverage Score Last 72 Episodes: 8.740\tAvg. Score (All Agents) Last Episode: 20.976\n",
      "Episode 73\tAverage Score Last 73 Episodes: 8.914\tAvg. Score (All Agents) Last Episode: 21.506\n",
      "Episode 74\tAverage Score Last 74 Episodes: 9.084\tAvg. Score (All Agents) Last Episode: 21.425\n",
      "Episode 75\tAverage Score Last 75 Episodes: 9.261\tAvg. Score (All Agents) Last Episode: 22.398\n",
      "Episode 76\tAverage Score Last 76 Episodes: 9.464\tAvg. Score (All Agents) Last Episode: 24.703\n",
      "Episode 77\tAverage Score Last 77 Episodes: 9.637\tAvg. Score (All Agents) Last Episode: 22.752\n",
      "Episode 78\tAverage Score Last 78 Episodes: 9.830\tAvg. Score (All Agents) Last Episode: 24.728\n",
      "Episode 79\tAverage Score Last 79 Episodes: 10.019\tAvg. Score (All Agents) Last Episode: 24.742\n",
      "Episode 80\tAverage Score Last 80 Episodes: 10.211\tAvg. Score (All Agents) Last Episode: 25.405\n",
      "Episode 81\tAverage Score Last 81 Episodes: 10.407\tAvg. Score (All Agents) Last Episode: 26.068\n",
      "Episode 82\tAverage Score Last 82 Episodes: 10.591\tAvg. Score (All Agents) Last Episode: 25.520\n",
      "Episode 83\tAverage Score Last 83 Episodes: 10.781\tAvg. Score (All Agents) Last Episode: 26.341\n",
      "Episode 84\tAverage Score Last 84 Episodes: 10.967\tAvg. Score (All Agents) Last Episode: 26.368\n",
      "Episode 85\tAverage Score Last 85 Episodes: 11.160\tAvg. Score (All Agents) Last Episode: 27.421\n",
      "Episode 86\tAverage Score Last 86 Episodes: 11.352\tAvg. Score (All Agents) Last Episode: 27.668\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-71fd59467add>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mall_code\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0msuccess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mavg_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscores_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mppo_continuous_unity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-43-c05ab1f87715>\u001b[0m in \u001b[0;36mppo_continuous_unity\u001b[1;34m()\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[1;31m# config.logger = get_logger()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[0magent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPPOAgent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mrun_steps_unity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-33-c80741224d8e>\u001b[0m in \u001b[0;36mrun_steps_unity\u001b[1;34m(agent)\u001b[0m\n\u001b[0;32m     24\u001b[0m             \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m         \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\MyDev\\deep learning\\unity-ml-reacher\\research\\all_code.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    112\u001b[0m         \u001b[0mstates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrollout_length\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m             \u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog_probs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m             \u001b[0mnext_states\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mterminals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0monline_rewards\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mrewards\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mI:\\MyDev\\Anaconda3\\envs\\drlnd\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    476\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 477\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    478\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\MyDev\\deep learning\\unity-ml-reacher\\research\\all_code.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, obs, action)\u001b[0m\n\u001b[0;32m    225\u001b[0m         \u001b[0mmean\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mphi_a\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m         \u001b[0mv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc_critic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mphi_v\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 227\u001b[1;33m         \u001b[0mdist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNormal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    228\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0maction\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m             \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mI:\\MyDev\\Anaconda3\\envs\\drlnd\\lib\\site-packages\\torch\\distributions\\normal.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, loc, scale, validate_args)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidate_args\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscale\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbroadcast_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNumber\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscale\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNumber\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mbatch_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mI:\\MyDev\\Anaconda3\\envs\\drlnd\\lib\\site-packages\\torch\\distributions\\utils.py\u001b[0m in \u001b[0;36mbroadcast_all\u001b[1;34m(*values)\u001b[0m\n\u001b[0;32m     74\u001b[0m         \u001b[0mbroadcast_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_broadcast_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtensor_idxs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtensor_idxs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m             \u001b[0mvalues\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbroadcast_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m         \u001b[0mtemplate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtensor_idxs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mscalar_idxs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from all_code import *\n",
    "success, avg_score, scores_list, path = ppo_continuous_unity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if success:\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    plt.plot(np.arange(1, len(scores_list)+1), scores_list)\n",
    "    plt.ylabel('Score')\n",
    "    plt.xlabel('Episode #')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_rl.network.network_heads import GaussianActorCriticNet\n",
    "from deep_rl.network.network_bodies import FCBody, DummyBody\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torchviz import make_dot\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FCBody(33)\n",
    "\n",
    "x = Variable(torch.randn(33)).unsqueeze(0)\n",
    "model.eval()\n",
    "y = model(x)\n",
    "             \n",
    "make_dot(y, params=dict(list(model.named_parameters())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "critic_model = FCBody(33)\n",
    "\n",
    "x = Variable(torch.randn(33)).unsqueeze(0)\n",
    "model.eval()\n",
    "y = model(x)\n",
    "             \n",
    "make_dot(y, params=dict(list(model.named_parameters())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor(5.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
