import numpy as np

import torch
import torch.nn.functional as F
import torch.optim as optim
from agents.model_ppo import Gaussian
import random
from agents.utils import soft_update


class PPO():
    """Interacts with and learns from the environment."""
    
    def __init__(self, state_size, action_size, seed, device, num_agents, 
                 LR=1e-4, 
                 GRADIENT_CLIP=1, 
                 EPOCHS=4, 
                 BATCH_SIZE=32,
                GAMMA=0.99,
                TAU=1e-3,
                CLIP_EPSILON=1e-1):
        self.seed = random.seed(seed)
        self.num_agents = num_agents
        self.device = device
        self.network = Gaussian(state_size, action_size, seed)
        # Should we dd eps on the optim to 1e-5?
        self.optim = optim.Adam(self.network.parameters(), lr=LR)
        self.GRADIENT_CLIP = GRADIENT_CLIP
        self.EPOCHS=EPOCHS
        self.BATCH_SIZE=BATCH_SIZE
        self.TAU=TAU
        self.CLIP_EPSILON=CLIP_EPSILON
    
    def save(self):
        torch.save(self.critic_local.state_dict(),"ppo.pth")
     
    def load(self, path):
        self.critic_local.load_state_dict(torch.load(path))
    
    def act(self, states):
        """ 
        """
        states = torch.tensor(states).float().to(self.device)
        ret = []
        actions, log_probs, values = self.network(states)
        return actions.cpu().detach(), log_probs.cpu().detach(), values.cpu().detach()
    
    def learn(self, states, actions, log_probs, values, rewards, next_states, dones):
        """
        Noise Reduction: Collect more trajectories and when computing the gradients, use the mean gradient accross all trajectories
        Credit Assignment: 
            instead of using the total sum of the reward, use the cumulated rewards
            gradient = SUM[ SUM(advantage*Gradient(log_prob(a|s) )]
        Importance Sampling -> Surrogate:
            Network theta -> generate action a for log_prob
            Network theta'-> find out how likely is the same action a to be generated by theta'
            Instead of gradient(log_prob(a|s)) use gradient(log_prob' - log_prob)*exp() = gradient(prob'/prob)
                This is about re-weighting factor
            Need to understand Importance Sampling and the how to apply it to probabilities
        Clip Surrogate
            apply a gradient which is always less tha 1+epsilon of the ration
        """
        states = torch.tensor(states).float().to(self.device)
        actions = torch.tensor(actions).float().to(self.device)
        log_probs = torch.tensor(log_probs).float().to(self.device)
        values = torch.tensor(values).float().to(self.device)
        rewards = torch.tensor(rewards).float().to(self.device)
        next_states = torch.tensor(next_states).float().to(self.device)
        dones = torch.tensor(dones).astype(np.uint8).float().to(self.device)
        
        # should the returns be 0 if the next_state is done?
        _, _, returns = self.network(next_states[-1])
        
        # this might be optimized with torch.cumsum
        advantages = []
        for i in reverse(range(states.shape[0])):
            # for Q(s,a) = V(s) + A(s,a)
            # ==> A(s,a) = Q(s,a) - V(s)
            
            returns = rewards[i] + self.GAMMA*returns*(1-dones)
            advantages.append((returns - values[i]).detach())
        advantages = torch.cat(reverse(advantages), dim=0)
        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-10)
        # prepare states, actions, returns,
        for _ in range(self.EPOCHS):
            # Shuffle the indices
            for indices in self.batch_indices(len(states), self.BATCH_SIZE):
                idx = tensor(indices).long()
                sampled_states = states[idx]
                sampled_actions = actions[idx]
                sampled_next_states = next_states[idx]
                sampled_rewards = rewards[idx]
                sampled_dones = dones[idx]                
                sampled_log_probs = log_probs[idx]
                sampled_advantages = advantages[idx]
                
                # find out how likely the new network would have chosen the sampled_actions
                _, new_log_probs, _ = self.network(sampled_states, sampled_actions)
                ratio = (new_log_probs - sampled_log_probs).exp()
                clip = torch.clamp(ratio, 1-self.CLIP_EPSILON, 1+self.CLIP_EPSILON)
                clipped_surrogate = torch.min(ratio*sampled_advantages, clip*sampled_advantages) 
                # the previous line is different from torch.min(ratio, clip)*sampled_advantages when sampled_advantages are negative
                
                # Udacity on PPO is having an entropy about left/right action that may be useful here
                # Should the mean be mean() Udacity since the rewards are normalized or mean(0) ShangTong?
                policy_loss = -clipped_surrogate.mean()
                
                # In ShangTong's implementation, the targeted value is the cumulated rewards which is in fact another form of n-step bootstrap
                # sampled_targeted_values = returns[indices] # Here returns correspond to r + gamma*r1 + r2*gamma**2 + .... + V(last_state)*gamma**n
                # in our case, it would be the normal TD as follow were V' is a copy of V which is updated through soft_update from V to V'
                # V(s)=r + gamma*V'(s')
                _, _, sampled_next_values = self.network.critic_target(sampled_next_states, None)
                sampled_targeted_values = rewards + self.GAMMA*sampled_next_states*(1-sampled_dones)
                
                # Ask the critic about the values of these states
                _, _, estimated_values = self.network(sampled_states, sampled_actions)
                value_loss = F.mse_loss(estimated_values, sampled_targeted_values)
                self.optim.zero_grad()
                (policy_loss + value_loss).backward()
                torch.nn.utils.clip_grad_norm_(self.critic_local.parameters(), self.GRADIENT_CLIP)
                self.optim.step()
                soft_update(self.network.critic, self.network.critic_target,self.TAU)
                
                del idx, sampled_states, sampled_actions, sampled_next_states
                del sampled_rewards, sampled_dones, sampled_log_probs, sampled_advantages
                del new_log_probs, ratio, clip, clipped_surrogate, estimated_values, clipped_surrogate, policy_loss, value_loss
        del states, actions, log_probs, values, rewards, next_states, dones, returns
    
    def batch_indices(self, length, batch_size):
        indices = np.arange(length)
        np.random.shuffler(indices)
        for i in range(1 + length // batch_size):
            start = batch_size*i
            end = start + batch_size
            end = min(length, end)
            yield indices[start:end]