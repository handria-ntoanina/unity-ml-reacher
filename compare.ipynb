{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from unityagents import UnityEnvironment\n",
    "from collections import deque\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from research.all_code import *\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name='./Reacher_Windows_x86_64/Reacher.exe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 20\n",
      "Size of each action: 4\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseTaskUnity:\n",
    "    def __init__(self, train_mode):\n",
    "        self.train_mode = train_mode\n",
    "        pass\n",
    "\n",
    "    def reset(self):\n",
    "        env_info = self.env.reset(train_mode=self.train_mode)[brain_name]\n",
    "        return np.array(env_info.vector_observations)\n",
    "\n",
    "    def step(self, action):\n",
    "        env_info = self.env.step(action)[brain_name]\n",
    "        next_states = env_info.vector_observations\n",
    "        rewards = env_info.rewards\n",
    "        dones = env_info.local_done\n",
    "        if np.any(dones):\n",
    "            next_states = self.reset()\n",
    "        return np.array(next_states), np.array(rewards), np.array(dones), None\n",
    "\n",
    "    def seed(self, random_seed):\n",
    "        pass\n",
    "\n",
    "    \n",
    "class ReacherV1(BaseTaskUnity):\n",
    "    def __init__(self, name, train_mode):\n",
    "        BaseTaskUnity.__init__(self, train_mode)\n",
    "        self.name = name\n",
    "        self.env = env\n",
    "        self.action_dim = brain.vector_action_space_size\n",
    "        self.state_dim = brain.vector_observation_space_size\n",
    "\n",
    "    def step(self, action):\n",
    "        return BaseTaskUnity.step(self, np.clip(action, -1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_steps_unity(agent):\n",
    "    random_seed()\n",
    "    torch.manual_seed(np.random.randint(int(1e6)))\n",
    "    config = agent.config\n",
    "    while True:\n",
    "        agent.step()\n",
    "        \n",
    "        scores = agent.scores_deque\n",
    "        mean_100 = np.mean(scores)\n",
    "        i_episode = len(agent.scores_list)\n",
    "        print('Episode {}\\tAverage Score: {:.3f}\\tLast Score: {:.3f}\\tMax Score: {:.3f}'.format(i_episode, \n",
    "                                                                                          mean_100, \n",
    "                                                                                          scores[-1],\n",
    "                                                                                         agent.score_max))\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.3f}\\tMax Avg Score: {:.3f}'.format(i_episode, mean_100, np.max(scores)))\n",
    "            save_path = 'PPO-ReacherV2-checkpoint.bin'\n",
    "            agent.save(save_path)\n",
    "        if len(scores) >= 100 and mean_100>=max_t*30.0/950:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.3f}'.format(i_episode, mean_100))\n",
    "            save_path = 'PPO-ReacherV2-solved.bin'\n",
    "            agent.save(save_path)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppo_continuous_unity(train_mode=True):\n",
    "    config = Config()\n",
    "    # task_fn = lambda log_dir: Pendulum(log_dir=log_dir)\n",
    "    # task_fn = lambda log_dir: Bullet('AntBulletEnv-v0', log_dir=log_dir)\n",
    "    task_fn = lambda: ReacherV1('ReacherV1',train_mode)\n",
    "    config.task_fn = task_fn\n",
    "    config.state_dim = 33\n",
    "    config.action_dim = 4\n",
    "\n",
    "    config.network_fn = lambda: GaussianActorCriticNet(\n",
    "        config.state_dim, config.action_dim, actor_body=FCBody(config.state_dim),\n",
    "        critic_body=FCBody(config.state_dim))\n",
    "    config.optimizer_fn = lambda params: torch.optim.Adam(params, 3e-4, eps=1e-5)\n",
    "    config.discount = 0.99\n",
    "    config.use_gae = True\n",
    "    config.gae_tau = 0.95\n",
    "    config.gradient_clip = 5\n",
    "    config.rollout_length = 256\n",
    "    config.optimization_epochs = 10\n",
    "    config.num_mini_batches = 256\n",
    "    config.ppo_ratio_clip = 0.2\n",
    "    config.log_interval = 2048\n",
    "    config.max_steps = 2e7\n",
    "    # config.logger = get_logger()\n",
    "    agent = PPOAgent(config)\n",
    "    return run_steps_unity(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1\tAverage Score: 0.020\tLast Score: 0.020\tMax Score: 0.410\n",
      "Episode 2\tAverage Score: 0.016\tLast Score: 0.011\tMax Score: 0.220\n",
      "Episode 3\tAverage Score: 0.012\tLast Score: 0.004\tMax Score: 0.080\n",
      "Episode 4\tAverage Score: 0.020\tLast Score: 0.046\tMax Score: 0.450\n",
      "Episode 5\tAverage Score: 0.037\tLast Score: 0.104\tMax Score: 0.720\n",
      "Episode 6\tAverage Score: 0.047\tLast Score: 0.093\tMax Score: 0.610\n",
      "Episode 7\tAverage Score: 0.056\tLast Score: 0.116\tMax Score: 0.740\n",
      "Episode 8\tAverage Score: 0.070\tLast Score: 0.168\tMax Score: 0.730\n",
      "Episode 9\tAverage Score: 0.082\tLast Score: 0.177\tMax Score: 0.710\n",
      "Episode 10\tAverage Score: 0.093\tLast Score: 0.186\tMax Score: 0.870\n",
      "Episode 11\tAverage Score: 0.102\tLast Score: 0.197\tMax Score: 1.130\n",
      "Episode 12\tAverage Score: 0.115\tLast Score: 0.255\tMax Score: 1.160\n",
      "Episode 13\tAverage Score: 0.113\tLast Score: 0.085\tMax Score: 0.430\n",
      "Episode 14\tAverage Score: 0.119\tLast Score: 0.202\tMax Score: 1.030\n",
      "Episode 15\tAverage Score: 0.123\tLast Score: 0.177\tMax Score: 0.740\n",
      "Episode 16\tAverage Score: 0.130\tLast Score: 0.228\tMax Score: 0.660\n",
      "Episode 17\tAverage Score: 0.139\tLast Score: 0.282\tMax Score: 0.590\n",
      "Episode 18\tAverage Score: 0.145\tLast Score: 0.248\tMax Score: 1.180\n",
      "Episode 19\tAverage Score: 0.154\tLast Score: 0.314\tMax Score: 1.310\n",
      "Episode 20\tAverage Score: 0.160\tLast Score: 0.290\tMax Score: 1.410\n",
      "Episode 21\tAverage Score: 0.175\tLast Score: 0.467\tMax Score: 1.410\n",
      "Episode 22\tAverage Score: 0.184\tLast Score: 0.373\tMax Score: 0.950\n",
      "Episode 23\tAverage Score: 0.190\tLast Score: 0.326\tMax Score: 1.020\n",
      "Episode 24\tAverage Score: 0.195\tLast Score: 0.306\tMax Score: 1.010\n",
      "Episode 25\tAverage Score: 0.199\tLast Score: 0.284\tMax Score: 0.790\n",
      "Episode 26\tAverage Score: 0.205\tLast Score: 0.360\tMax Score: 1.000\n",
      "Episode 27\tAverage Score: 0.205\tLast Score: 0.207\tMax Score: 0.600\n",
      "Episode 28\tAverage Score: 0.213\tLast Score: 0.428\tMax Score: 1.950\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-18da013ad505>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mresearch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall_code\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0msuccess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mavg_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscores_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mppo_continuous_unity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-22-67ec6d9504d0>\u001b[0m in \u001b[0;36mppo_continuous_unity\u001b[1;34m(train_mode)\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[1;31m# config.logger = get_logger()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[0magent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPPOAgent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mrun_steps_unity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-21-72678f31d280>\u001b[0m in \u001b[0;36mrun_steps_unity\u001b[1;34m(agent)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mconfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscores_deque\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\MyDev\\deep learning\\unity-ml-reacher\\research\\all_code.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m                 \u001b[1;33m(\u001b[0m\u001b[0mpolicy_loss\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mvalue_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m                 \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient_clip\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mI:\\MyDev\\Anaconda3\\envs\\drlnd\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m         \"\"\"\n\u001b[1;32m---> 93\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mI:\\MyDev\\Anaconda3\\envs\\drlnd\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from research.all_code import *\n",
    "success, avg_score, scores_list, path = ppo_continuous_unity(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1\tAverage Score: 0.002\tLast Score: 0.002\tMax Score: 0.050\n",
      "Episode 2\tAverage Score: 0.004\tLast Score: 0.006\tMax Score: 0.130\n",
      "Episode 3\tAverage Score: 0.010\tLast Score: 0.020\tMax Score: 0.260\n",
      "Episode 4\tAverage Score: 0.011\tLast Score: 0.015\tMax Score: 0.270\n",
      "Episode 5\tAverage Score: 0.016\tLast Score: 0.034\tMax Score: 0.570\n",
      "Episode 6\tAverage Score: 0.027\tLast Score: 0.083\tMax Score: 1.140\n",
      "Episode 7\tAverage Score: 0.033\tLast Score: 0.065\tMax Score: 0.430\n",
      "Episode 8\tAverage Score: 0.040\tLast Score: 0.093\tMax Score: 0.440\n",
      "Episode 9\tAverage Score: 0.073\tLast Score: 0.339\tMax Score: 1.910\n",
      "Episode 10\tAverage Score: 0.078\tLast Score: 0.116\tMax Score: 0.520\n",
      "Episode 11\tAverage Score: 0.096\tLast Score: 0.279\tMax Score: 1.020\n",
      "Episode 12\tAverage Score: 0.109\tLast Score: 0.257\tMax Score: 0.920\n",
      "Episode 13\tAverage Score: 0.127\tLast Score: 0.337\tMax Score: 0.980\n",
      "Episode 14\tAverage Score: 0.142\tLast Score: 0.330\tMax Score: 1.070\n",
      "Episode 15\tAverage Score: 0.146\tLast Score: 0.213\tMax Score: 0.690\n",
      "Episode 16\tAverage Score: 0.153\tLast Score: 0.255\tMax Score: 0.760\n",
      "Episode 17\tAverage Score: 0.153\tLast Score: 0.158\tMax Score: 0.640\n",
      "Episode 18\tAverage Score: 0.165\tLast Score: 0.363\tMax Score: 1.840\n",
      "Episode 19\tAverage Score: 0.168\tLast Score: 0.215\tMax Score: 0.730\n",
      "Episode 20\tAverage Score: 0.172\tLast Score: 0.248\tMax Score: 0.590\n",
      "Episode 21\tAverage Score: 0.187\tLast Score: 0.495\tMax Score: 1.670\n",
      "Episode 22\tAverage Score: 0.199\tLast Score: 0.455\tMax Score: 1.110\n",
      "Episode 23\tAverage Score: 0.208\tLast Score: 0.396\tMax Score: 1.000\n",
      "Episode 24\tAverage Score: 0.215\tLast Score: 0.374\tMax Score: 0.970\n",
      "Episode 25\tAverage Score: 0.221\tLast Score: 0.368\tMax Score: 1.720\n",
      "Episode 26\tAverage Score: 0.227\tLast Score: 0.367\tMax Score: 1.360\n",
      "Episode 27\tAverage Score: 0.236\tLast Score: 0.481\tMax Score: 1.220\n",
      "Episode 28\tAverage Score: 0.241\tLast Score: 0.368\tMax Score: 1.360\n",
      "Episode 29\tAverage Score: 0.247\tLast Score: 0.412\tMax Score: 1.010\n",
      "Episode 30\tAverage Score: 0.251\tLast Score: 0.367\tMax Score: 0.700\n",
      "Episode 31\tAverage Score: 0.255\tLast Score: 0.384\tMax Score: 1.330\n",
      "Episode 32\tAverage Score: 0.257\tLast Score: 0.321\tMax Score: 1.450\n",
      "Episode 33\tAverage Score: 0.266\tLast Score: 0.537\tMax Score: 1.220\n",
      "Episode 34\tAverage Score: 0.279\tLast Score: 0.720\tMax Score: 1.580\n",
      "Episode 35\tAverage Score: 0.284\tLast Score: 0.453\tMax Score: 2.170\n",
      "Episode 36\tAverage Score: 0.292\tLast Score: 0.592\tMax Score: 1.280\n",
      "Episode 37\tAverage Score: 0.296\tLast Score: 0.421\tMax Score: 1.170\n",
      "Episode 38\tAverage Score: 0.301\tLast Score: 0.493\tMax Score: 1.260\n",
      "Episode 39\tAverage Score: 0.307\tLast Score: 0.525\tMax Score: 1.770\n",
      "Episode 40\tAverage Score: 0.309\tLast Score: 0.406\tMax Score: 1.880\n",
      "Episode 41\tAverage Score: 0.312\tLast Score: 0.420\tMax Score: 1.450\n",
      "Episode 42\tAverage Score: 0.313\tLast Score: 0.346\tMax Score: 0.820\n",
      "Episode 43\tAverage Score: 0.320\tLast Score: 0.632\tMax Score: 1.850\n",
      "Episode 44\tAverage Score: 0.323\tLast Score: 0.418\tMax Score: 1.070\n",
      "Episode 45\tAverage Score: 0.331\tLast Score: 0.688\tMax Score: 1.330\n",
      "Episode 46\tAverage Score: 0.341\tLast Score: 0.789\tMax Score: 1.650\n",
      "Episode 47\tAverage Score: 0.345\tLast Score: 0.524\tMax Score: 1.280\n",
      "Episode 48\tAverage Score: 0.351\tLast Score: 0.670\tMax Score: 1.460\n",
      "Episode 49\tAverage Score: 0.353\tLast Score: 0.428\tMax Score: 1.190\n",
      "Episode 50\tAverage Score: 0.354\tLast Score: 0.425\tMax Score: 1.430\n",
      "Episode 51\tAverage Score: 0.360\tLast Score: 0.653\tMax Score: 1.600\n",
      "Episode 52\tAverage Score: 0.367\tLast Score: 0.711\tMax Score: 1.440\n",
      "Episode 53\tAverage Score: 0.372\tLast Score: 0.612\tMax Score: 1.230\n",
      "Episode 54\tAverage Score: 0.379\tLast Score: 0.791\tMax Score: 1.640\n",
      "Episode 55\tAverage Score: 0.382\tLast Score: 0.506\tMax Score: 1.160\n",
      "Episode 56\tAverage Score: 0.385\tLast Score: 0.580\tMax Score: 1.190\n",
      "Episode 57\tAverage Score: 0.396\tLast Score: 1.002\tMax Score: 2.010\n",
      "Episode 58\tAverage Score: 0.405\tLast Score: 0.887\tMax Score: 2.440\n",
      "Episode 59\tAverage Score: 0.410\tLast Score: 0.752\tMax Score: 3.190\n",
      "Episode 60\tAverage Score: 0.415\tLast Score: 0.677\tMax Score: 1.570\n",
      "Episode 61\tAverage Score: 0.422\tLast Score: 0.867\tMax Score: 1.980\n",
      "Episode 62\tAverage Score: 0.427\tLast Score: 0.713\tMax Score: 1.860\n",
      "Episode 63\tAverage Score: 0.434\tLast Score: 0.877\tMax Score: 2.020\n",
      "Episode 64\tAverage Score: 0.438\tLast Score: 0.691\tMax Score: 1.510\n",
      "Episode 65\tAverage Score: 0.442\tLast Score: 0.700\tMax Score: 1.860\n",
      "Episode 66\tAverage Score: 0.450\tLast Score: 0.990\tMax Score: 2.010\n",
      "Episode 67\tAverage Score: 0.456\tLast Score: 0.837\tMax Score: 1.990\n",
      "Episode 68\tAverage Score: 0.462\tLast Score: 0.832\tMax Score: 2.700\n",
      "Episode 69\tAverage Score: 0.469\tLast Score: 0.972\tMax Score: 2.540\n",
      "Episode 70\tAverage Score: 0.475\tLast Score: 0.845\tMax Score: 1.970\n",
      "Episode 71\tAverage Score: 0.482\tLast Score: 0.993\tMax Score: 3.490\n",
      "Episode 72\tAverage Score: 0.488\tLast Score: 0.926\tMax Score: 1.910\n",
      "Episode 73\tAverage Score: 0.492\tLast Score: 0.788\tMax Score: 2.120\n",
      "Episode 74\tAverage Score: 0.499\tLast Score: 0.986\tMax Score: 2.380\n",
      "Episode 75\tAverage Score: 0.504\tLast Score: 0.876\tMax Score: 2.750\n",
      "Episode 76\tAverage Score: 0.510\tLast Score: 0.976\tMax Score: 2.250\n",
      "Episode 77\tAverage Score: 0.517\tLast Score: 1.028\tMax Score: 1.880\n",
      "Episode 78\tAverage Score: 0.522\tLast Score: 0.915\tMax Score: 1.890\n",
      "Episode 79\tAverage Score: 0.525\tLast Score: 0.798\tMax Score: 2.240\n",
      "Episode 80\tAverage Score: 0.525\tLast Score: 0.508\tMax Score: 1.150\n",
      "Episode 81\tAverage Score: 0.528\tLast Score: 0.770\tMax Score: 2.180\n",
      "Episode 82\tAverage Score: 0.532\tLast Score: 0.849\tMax Score: 1.780\n",
      "Episode 83\tAverage Score: 0.538\tLast Score: 1.010\tMax Score: 2.600\n",
      "Episode 84\tAverage Score: 0.543\tLast Score: 0.941\tMax Score: 1.900\n",
      "Episode 85\tAverage Score: 0.547\tLast Score: 0.946\tMax Score: 1.890\n",
      "Episode 86\tAverage Score: 0.554\tLast Score: 1.140\tMax Score: 2.810\n",
      "Episode 87\tAverage Score: 0.561\tLast Score: 1.117\tMax Score: 2.370\n"
     ]
    }
   ],
   "source": [
    "from agents.ppo import PPO\n",
    "from agents.model_ppo import Gaussian\n",
    "import random\n",
    "\n",
    "random_seed()\n",
    "device = \"cpu\"\n",
    "states = env_info.vector_observations\n",
    "action_size = brain.vector_action_space_size\n",
    "network = Gaussian(states.shape[1], action_size).to(device)\n",
    "agent = PPO(network, device,\n",
    "                 LR=3e-4,\n",
    "                 GRADIENT_CLIP=5, \n",
    "                 EPOCHS=10, \n",
    "                 BATCH_SIZE=256,\n",
    "                GAMMA=0.99,\n",
    "                GAE_TAU=0.95,\n",
    "                CLIP_EPSILON=0.2)\n",
    "scores = train(agent, n_episodes=6000, max_t=256, train_mode=True)\n",
    "plot_result(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(agent, n_episodes=500, max_t=500, train_mode=True):\n",
    "    scores = []                        # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        env_info = env.reset(train_mode=train_mode)[brain_name]\n",
    "        states = env_info.vector_observations\n",
    "        scores_one_episode = np.zeros(states.shape[0])\n",
    "        trajectories_states, trajectories_actions, trajectories_log_probs, trajectories_values, \\\n",
    "        trajectories_rewards, trajectories_next_states, trajectories_dones = [],[],[],[],[],[],[]\n",
    "        \n",
    "        for t in range(max_t):\n",
    "            actions, log_probs, values = agent.act(states)\n",
    "            env_info = env.step(np.clip(actions, -1, 1))[brain_name] # send all actions to the environment\n",
    "            next_states = env_info.vector_observations               # get next state (for each agent)\n",
    "            rewards = env_info.rewards                               # get reward (for each agent)\n",
    "            dones = env_info.local_done                              # see if episode finished\n",
    "            scores_one_episode += rewards\n",
    "            trajectories_states.append(states)\n",
    "            trajectories_actions.append(actions)\n",
    "            trajectories_log_probs.append(log_probs)\n",
    "            trajectories_values.append(values)\n",
    "            trajectories_rewards.append(rewards)\n",
    "            trajectories_next_states.append(next_states)\n",
    "            trajectories_dones.append(dones)\n",
    "            states = next_states                                     # roll over states to next time step\n",
    "            if np.any(dones):                                        # exit loop if episode finished\n",
    "                break\n",
    "  \n",
    "        agent.learn(trajectories_states, trajectories_actions, trajectories_log_probs, trajectories_values, \n",
    "        trajectories_rewards, trajectories_next_states, trajectories_dones)\n",
    "        score = np.mean(scores_one_episode)\n",
    "        scores.append(score)\n",
    "        scores_window.append(score)\n",
    "        mean_100 = np.mean(scores_window)\n",
    "        print('Episode {}\\tAverage Score: {:.3f}\\tLast Score: {:.3f}\\tMax Score: {:.3f}'.format(i_episode, \n",
    "                                                                                          mean_100, \n",
    "                                                                                          score,\n",
    "                                                                                         np.max(scores_one_episode)))\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.3f}\\tMax Avg Score: {:.3f}'.format(i_episode, mean_100, np.max(scores_window)))\n",
    "            agent.save()\n",
    "        if len(scores_window) >= 100 and np.mean(scores_window)>=max_t*30.0/950:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.3f}'.format(i_episode, mean_100))\n",
    "            agent.save()\n",
    "            break\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
